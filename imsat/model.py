import tensorflow as tf
from tensorflow.contrib.layers import fully_connected
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import tensor_array_ops
from tensorflow.python.ops.rnn_cell_impl import LSTMStateTuple

START_WORD_INDEX = 1
END_WORD_INDEX = 2
CAPTION_MAX_LENGTH = 16


def create_loss(outputs, captions, length):
  with tf.variable_scope('loss'):
    outputs = _align_text(captions, outputs)

    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(
      logits=outputs[:, :-1, :], labels=captions[:, 1:])
    loss = _mask_loss(length, losses)
  return loss


def _mask_loss(length, losses):
  '''
  Mask losses with the length tensor.
  :param length: The length tensor which refer to the length
    of captions.
  :param losses: cross entropy of between outputs and captions
  :return: summed of masked losses
  '''
  losses_length = tf.shape(losses)[1]
  loss_mask = tf.sequence_mask(
    tf.to_int32(length), losses_length)
  losses = losses * tf.to_float(loss_mask)
  loss = tf.reduce_sum(losses) / tf.to_float(tf.reduce_sum(length - 1))
  return loss


def _align_text(captions, outputs):
  '''
  Mad outputs to make them same length to captions
  :param captions: captions to be aligned.
  :param outputs: outputs generated by decoder
  :return: padded outputs
  '''
  bucket_size = tf.shape(captions)[1]
  output_len = tf.shape(outputs)[1]
  # 1st dimension of indices is the No. of dimension to update,
  # since the output shape is (batch, time, word), we should
  # pad on the second dimension, which is `time`.
  indexes = [[1, 1]]
  values = tf.expand_dims(bucket_size - output_len, axis=0)
  # because rank of final outputs tensor is 3, so the shape is (3, 2)
  # for example, output shape is (2, 4, 1000), and caption shape is
  # (2, 6), we should pad the second dimension to 6. So the padding
  # matrix should be ((0, 0), (0, 2), (0, 0))
  shape = [3, 2]
  paddings = tf.scatter_nd(indexes, values, shape)
  outputs = tf.pad(outputs, paddings)
  return outputs


def _batch_norm(x, mode='train', name=None):
  return tf.contrib.layers.batch_norm(inputs=x,
                                      decay=0.95,
                                      center=True,
                                      scale=True,
                                      is_training=(mode == 'train'),
                                      updates_collections=None,
                                      scope=(name + 'batch_norm'))


class AttendTell:
  def __init__(self,
               vocab_size,
               dim_feature=(196, 512),
               dim_embed=512,
               dim_hidden=1024,
               prev2out=True,
               ctx2out=True,
               alpha_c=0.0,
               selector=True,
               dropout=True):
    self.prev2out = prev2out
    self.ctx2out = ctx2out
    self.alpha_c = alpha_c
    self.selector = selector
    self.dropout = dropout
    self.vocab_size = vocab_size
    self.position_num = dim_feature[0]
    self.feature_length = dim_feature[1]
    self.embedding_size = dim_embed
    self.hidden_size = dim_hidden

    self.weight_initializer = tf.contrib.layers.xavier_initializer()
    self.const_initializer = tf.constant_initializer(0.0)
    self.emb_initializer = tf.random_uniform_initializer(minval=-1.0,
                                                         maxval=1.0)

  def build(self, features, captions):
    cap_shape = tf.shape(captions)
    bucket_size = cap_shape[1]
    x = self._word_embedding(inputs=captions)

    features = _batch_norm(features, mode='train', name='conv_features')
    c, h = self._get_initial_lstm(features=features)
    # (batch, position_num, feature_size)
    features_proj = self._project_features(features=features)

    # TODO: Try other structures
    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=self.hidden_size)

    def condition(time, all_outputs: tf.TensorArray, caps, states):
      def has_end_word(t):
        return tf.reduce_any(tf.equal(t, END_WORD_INDEX))

      def check_all_ends():
        word_indexes = tf.argmax(all_outputs.stack(), axis=2)
        word_indexes = tf.transpose(word_indexes, [1, 0])
        end_word_flags = tf.map_fn(has_end_word, word_indexes, dtype=tf.bool)
        check_res = tf.reduce_all(end_word_flags)
        return check_res

      all_outputs_size = all_outputs.size()
      is_first_frame = tf.equal(all_outputs_size, 0)
      gen_ends = tf.cond(is_first_frame,
                         lambda: tf.constant(False, tf.bool),
                         check_all_ends)
      cond_res = tf.logical_and(tf.logical_not(gen_ends),
                                tf.less(time, bucket_size))
      return cond_res

    # inputs shape: (batch, embedding)
    # output shape: (batch, hidden_size)
    # hidden layer shape: (embedding, hidden_size)
    # h: (batch, hidden_size)
    def body(time, all_outputs: tf.TensorArray, caps, state: LSTMStateTuple):
      inputs = caps[:, time, :]
      # todo: attention layer missing here

      # context: (batch, feature_size)
      # alpha: (batch, position_num)
      context, alpha = self._attention_layer(features, features_proj, state.h)

      # todo: alpha regularization
      # todo: selector

      # decoder_input: (batch, embedding_size + feature_size)
      decoder_input = tf.concat(values=[inputs, context], axis=1)
      output, nxt_state = lstm_cell(decoder_input, state=state)
      # todo: more complex decode lstm output
      logits = fully_connected(inputs=output,
                               num_outputs=self.vocab_size,
                               activation_fn=None)
      all_outputs = all_outputs.write(time, logits)
      return time + 1, all_outputs, caps, nxt_state

    out_ta = tensor_array_ops.TensorArray(tf.float32,
                                          size=0,
                                          dynamic_size=True,
                                          clear_after_read=False,
                                          element_shape=(None, self.vocab_size))
    init_state = LSTMStateTuple(c, h)

    _, outputs, _, _ = control_flow_ops.while_loop(
      condition,
      body,
      loop_vars=[0, out_ta, x, init_state]
    )
    outputs = tf.transpose(outputs.stack(), (1, 0, 2))
    return outputs

  def _attention_layer(self, features, features_proj, h):
    with tf.variable_scope('attention_layer'):
      state_proj = fully_connected(inputs=h,
                                   num_outputs=self.feature_length,
                                   activation_fn=None,
                                   weights_initializer=self.weight_initializer,
                                   biases_initializer=self.const_initializer)
      # todo:
      #   why `add` two projected feature here?
      #   I highly suspect we should call `multiply` here
      # feature_proj is (batch_size, posision_num, feature_size)
      # tf.expand_dims(tf.matmul(h, w), 1) + b) is (batch, 1, feature_size)
      h_att = tf.nn.relu(features_proj + tf.expand_dims(state_proj, 1))

      # this is just a linear regression
      flat_h_att = tf.reshape(h_att, [-1, self.feature_length])
      flat_att = fully_connected(inputs=flat_h_att,
                                 num_outputs=1,
                                 activation_fn=None,
                                 weights_initializer=self.weight_initializer,
                                 biases_initializer=None)
      out_att = tf.reshape(flat_att, [-1, self.position_num])  # (N, L)
      alpha = tf.nn.softmax(out_att)
      # context: (batch, feature_length)
      context = tf.reduce_sum(features * tf.expand_dims(alpha, 2),
                              axis=1,
                              name='context')
      return context, alpha

  def _get_initial_lstm(self, features):
    with tf.variable_scope('initial_lstm'):
      features_mean = tf.reduce_mean(features, 1)

      h = fully_connected(inputs=features_mean,
                          num_outputs=self.hidden_size,
                          activation_fn=tf.nn.tanh,
                          weights_initializer=self.weight_initializer,
                          biases_initializer=self.const_initializer)

      c = fully_connected(inputs=features_mean,
                          num_outputs=self.hidden_size,
                          activation_fn=tf.nn.tanh,
                          weights_initializer=self.weight_initializer,
                          biases_initializer=self.const_initializer)
      return c, h

  def _word_embedding(self, inputs):
    with tf.variable_scope('word_embedding'), tf.device("/cpu:0"):
      w = tf.get_variable('w',
                          [self.vocab_size, self.embedding_size],
                          initializer=self.emb_initializer)
      x = tf.nn.embedding_lookup(w, inputs, name='word_vector')
      return x

  # todo: I think this function has some issue. what is this function
  #   used for ?
  def _project_features(self, features):
    with tf.variable_scope('project_features'):
      features_flat = tf.reshape(features, [-1, self.feature_length])
      features_proj = fully_connected(
        inputs=features_flat,
        num_outputs=self.feature_length,
        activation_fn=None,
        weights_initializer=self.weight_initializer,
        biases_initializer=None
      )
      features_proj = tf.reshape(features_proj,
                                 [-1, self.position_num, self.feature_length])
      return features_proj

  def _decode_lstm(self, x, h, context, dropout=False, reuse=False):
    with tf.variable_scope('logits', reuse=reuse):
      w_h = tf.get_variable('w_h',
                            [self.hidden_size, self.embedding_size],
                            initializer=self.weight_initializer)
      b_h = tf.get_variable('b_h',
                            [self.embedding_size],
                            initializer=self.const_initializer)
      w_out = tf.get_variable('w_out',
                              [self.embedding_size, self.vocab_size],
                              initializer=self.weight_initializer)
      b_out = tf.get_variable('b_out',
                              [self.vocab_size],
                              initializer=self.const_initializer)

      if dropout:
        h = tf.nn.dropout(h, 0.5)
      h_logits = tf.matmul(h, w_h) + b_h

      if self.ctx2out:
        w_ctx2out = tf.get_variable('w_ctx2out',
                                    [self.D, self.M],
                                    initializer=self.weight_initializer)
        h_logits += tf.matmul(context, w_ctx2out)

      if self.prev2out:
        h_logits += x
      h_logits = tf.nn.tanh(h_logits)

      if dropout:
        h_logits = tf.nn.dropout(h_logits, 0.5)
      out_logits = tf.matmul(h_logits, w_out) + b_out
      return out_logits
